Tr√®s bien ! Voici le **code final complet**, mis √† jour pour fonctionner avec une **image r√©elle** et un **vocabulaire √©tendu** (lettres majuscules, minuscules, chiffres, accents, ponctuation).

---

## ‚úÖ OCR avec vocabulaire complet et image r√©elle ‚Äì Code Python pr√™t √† l'emploi

### üî† Vocabulaire utilis√© :

Inclut :
`a-z`, `A-Z`, `0-9`, accents (`√©√®√†√ß√π`), et ponctuation (`!?.,:;-_()`)

---

### üìÅ Fichier : `ocr_image_inference.py`

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
import cv2

# === PARAM√àTRES ===
img_height = 32
img_width = 128
vocab = list("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789√©√®√†√ß√π!?.,:;-_() ")
vocab_size = len(vocab)

# === 1. CHARGER UNE IMAGE ===
def load_image(filepath, img_height=32, img_width=128):
    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)
    img = cv2.resize(img, (img_width, img_height))
    img = img.astype(np.float32) / 255.0
    img = np.expand_dims(img, axis=-1)  # canal unique
    img = np.expand_dims(img, axis=0)   # batch size 1
    return img

# === 2. MOD√àLE OCR (CNN + RNN + CTC) ===
def build_ocr_model(input_shape, vocab_size):
    input_img = layers.Input(shape=input_shape, name='image')

    # CNN
    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(input_img)
    x = layers.MaxPooling2D((2,2))(x)
    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)
    x = layers.MaxPooling2D((2,2))(x)

    # RNN
    new_shape = (img_width // 4, (img_height // 4) * 64)
    x = layers.Reshape(target_shape=new_shape)(x)
    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)
    x = layers.Dense(vocab_size + 1, activation='softmax')(x)  # +1 pour CTC blank

    return models.Model(inputs=input_img, outputs=x)

# === 3. D√âCODAGE PR√âDICTION ===
def predict_text(model, image, vocab, img_width):
    y_pred = model.predict(image)
    decoded, _ = tf.keras.backend.ctc_decode(y_pred, input_length=[img_width // 4])
    result = ''.join([vocab[i] if i < len(vocab) else '' for i in decoded[0][0].numpy()])
    return result

# === 4. SCRIPT PRINCIPAL ===
if __name__ == "__main__":
    image_path = "image_test.png"  # <-- change ce chemin vers ton image r√©elle

    # Charger image
    image = load_image(image_path)

    # Construire le mod√®le (non entra√Æn√© ici)
    model = build_ocr_model((img_height, img_width, 1), vocab_size)

    # ‚ö†Ô∏è R√©sultat al√©atoire car mod√®le non entra√Æn√© !
    predicted_text = predict_text(model, image, vocab, img_width)

    # Affichage image + texte pr√©dit
    print("Texte pr√©dit :", predicted_text)
    plt.imshow(image[0].squeeze(), cmap='gray')
    plt.title(f"Texte pr√©dit : {predicted_text}")
    plt.axis('off')
    plt.show()
```

---

### ‚ö†Ô∏è Important :

Ce mod√®le **doit √™tre entra√Æn√©** sur un vrai dataset pour donner des r√©sultats valables. Actuellement, il pr√©dit **n‚Äôimporte quoi** (car les poids sont al√©atoires).

---

Souhaites-tu que je t‚Äôaide ensuite √† :

- Entra√Æner ce mod√®le avec un vrai jeu de donn√©es (IAM, SynthText, etc.) ?
- Ou int√©grer **Tesseract OCR** pour extraire du texte sans entra√Ænement ?

---

Bien s√ªr ! Pour concevoir un mod√®le OCR performant en utilisant le dataset SynthText, voici un guide d√©taill√© pour :

1. **T√©l√©charger et pr√©parer le dataset SynthText**.
2. **Construire un mod√®le OCR bas√© sur CNN + LSTM + CTC avec TensorFlow**.
3. **Entra√Æner le mod√®le sur le dataset SynthText**.

---

## 1. T√©l√©charger et pr√©parer le dataset SynthText

Le dataset SynthText est une vaste collection d'images contenant du texte synth√©tique, id√©ale pour l'entra√Ænement de mod√®les OCR.

### √âtapes :

1. **T√©l√©chargement du dataset** :

   - T√©l√©chargez le fichier `SynthText.zip` depuis [ce lien](https://github.com/ankush-me/SynthText/issues/114).
   - D√©compressez-le dans un r√©pertoire, par exemple `SynthText/`.

2. **T√©l√©chargement des annotations** :

   - T√©l√©chargez le fichier `label.txt` (ou `shuffle_labels.txt` pour un sous-ensemble) depuis [ce lien](https://github.com/ankush-me/SynthText/issues/114).
   - Placez-le dans le r√©pertoire `SynthText/`.

3. **Conversion des annotations** :

   Utilisez le script `synthtext_converter.py` pour convertir les annotations en un format utilisable pour l'entra√Ænement :

   ```bash
   python tools/data/textrecog/synthtext_converter.py SynthText/gt.mat SynthText/ SynthText/synthtext/SynthText_patch_horizontal --n_proc 8
   ```

   Cette commande g√©n√©rera des images d√©coup√©es et les associera √† leurs annotations correspondantes.

---

## 2. Construire le mod√®le OCR avec TensorFlow

Nous allons utiliser un mod√®le bas√© sur des couches CNN pour l'extraction de caract√©ristiques, suivi de couches LSTM pour la s√©quence, et une couche Dense avec une fonction de perte CTC pour la reconnaissance du texte.

### Code du mod√®le :

```python
import tensorflow as tf
from tensorflow.keras import layers, models

def build_ocr_model(input_shape, vocab_size):
    input_img = layers.Input(shape=input_shape, name='image')

    # Couches CNN
    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
    x = layers.MaxPooling2D((2, 2))(x)
    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling2D((2, 2))(x)

    # RNN avec LSTM
    new_shape = (input_shape[1] // 4, (input_shape[0] // 4) * 64)
    x = layers.Reshape(target_shape=new_shape)(x)
    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)

    # Couche Dense avec activation softmax
    x = layers.Dense(vocab_size + 1, activation='softmax')(x)  # +1 pour le "blank" du CTC

    return models.Model(inputs=input_img, outputs=x)
```

---

## 3. Entra√Æner le mod√®le sur SynthText

### Pr√©paration des donn√©es :

- **Images** : Les images d√©coup√©es sont stock√©es dans `SynthText/synthtext/SynthText_patch_horizontal/`.
- **Annotations** : Le fichier `label.txt` contient les transcriptions correspondantes.

### Chargement des donn√©es :

Utilisez une biblioth√®que comme `tf.data` pour charger les images et leurs annotations de mani√®re efficace.

### Fonction de perte CTC :

```python
def ctc_loss_fn(y_true, y_pred, input_length, label_length):
    return tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)
```

### Entra√Ænement :

```python
# Param√®tres
img_height, img_width = 32, 128
vocab = list("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789√©√®√†√ß√π!?.,:;-_() ")
vocab_size = len(vocab)
batch_size = 32
epochs = 10

# Construction du mod√®le
model = build_ocr_model((img_height, img_width, 1), vocab_size)

# Compilation du mod√®le
optimizer = tf.keras.optimizers.Adam()
model.compile(optimizer=optimizer, loss=ctc_loss_fn)

# Entra√Ænement (√† adapter selon votre m√©thode de chargement des donn√©es)
model.fit(train_dataset, epochs=epochs, validation_data=val_dataset)
```

---

## 4. √âvaluation et utilisation du mod√®le

Apr√®s l'entra√Ænement, vous pouvez √©valuer le mod√®le sur un jeu de donn√©es de test et l'utiliser pour pr√©dire du texte √† partir d'images.

### Pr√©diction :

```python
def predict_text(model, image, vocab, img_width):
    y_pred = model.predict(image)
    decoded, _ = tf.keras.backend.ctc_decode(y_pred, input_length=[img_width // 4])
    result = ''.join([vocab[i] if i < len(vocab) else '' for i in decoded[0][0].numpy()])
    return result
```

### Affichage du texte pr√©dit :

```python
image = load_image("image_test.png")
predicted_text = predict_text(model, image, vocab, img_width)
print("Texte pr√©dit :", predicted_text)
```

---

## 5. Ressources suppl√©mentaires

- **MMOCR** : Une bo√Æte √† outils OCR bas√©e sur PyTorch, compatible avec SynthText. Documentation : [https://mmocr.readthedocs.io/en/v0.3.0/datasets/recog.html](https://mmocr.readthedocs.io/en/v0.3.0/datasets/recog.html)
- **EasyOCR** : Une biblioth√®que OCR facile √† utiliser, compatible avec TensorFlow. Tutoriel : [https://www.freecodecamp.org/news/how-to-fine-tune-easyocr-with-a-synthetic-dataset](https://www.freecodecamp.org/news/how-to-fine-tune-easyocr-with-a-synthetic-dataset)

---

Souhaitez-vous que je vous aide √† adapter ce code pour votre propre environnement ou √† int√©grer des fonctionnalit√©s sp√©cifiques ?
